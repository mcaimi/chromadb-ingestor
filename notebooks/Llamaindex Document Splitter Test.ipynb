{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd7c6c-1767-4679-8e86-715c5e9e1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb==1.0.7 llama-index llama-index-core llama-index-embeddings-huggingface tf-keras llama-index-vector-stores-chroma\n",
    "!pip list | grep -e \"index-core\" -e \"index-embeddings\" -e \"chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe15f58-f5a2-4dc2-938f-c577f00e4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents and test the Document Loader class\n",
    "datasource_path: str = \"/tmp/data_path/\"\n",
    "text_data: str = \".txt\"\n",
    "\n",
    "print(f\"Text Data Path is: {datasource_path}, extensions are: {text_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6867c-f821-4046-b9e6-4c7c1c0e14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    from llama_index.core.ingestion import IngestionPipeline\n",
    "    from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "    from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "    from chromadb import Client, Collection\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02dd21-b2a0-4215-9029-31adfed1cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the directory with the llamaindex loader \n",
    "loader: SimpleDirectoryReader = SimpleDirectoryReader(input_dir=datasource_path, required_exts=[text_data])\n",
    "\n",
    "# ok, what's inside?\n",
    "data = loader.load_data()\n",
    "print(f\"Number of document loaded: {len(data)}\")\n",
    "print(f\" -> Each document is of type: {type(data[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6889e-09a5-4e66-b564-e2b61b2f2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Documents\n",
    "for doc in data:\n",
    "    print(f\"{doc.doc_id}, {doc.embedding}, {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc7b95-1fc4-4e3d-adc0-566b6a40873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a local embedding function using huggingface embedder\n",
    "embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "hf_embedder = HuggingFaceEmbedding(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd12bb-4bd0-4ac6-b85c-625c9348821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local in-memory instance of ChromaDB\n",
    "collection: str = \"jupyter\"\n",
    "chroma_client: Client = Client()\n",
    "chroma_collection: Collection = chroma_client.get_or_create_collection(collection,  metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store: ChromaVectorStore = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# ok vector db available\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059fd5f-1a7c-4bab-8329-1552f9c90573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate ingestion pipeline \n",
    "txt_pipe: IngestionPipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(embed_model=hf_embedder),\n",
    "        hf_embedder,\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196568e-a138-4b8d-bf78-55fb06e85051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pipeline!\n",
    "res = txt_pipe.run(documents=data)\n",
    "print(f\"Ingested {len(res)} semantically chunked documents\")\n",
    "print(f\"Vector DB contains {chroma_collection.count()} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b01d5a-63cd-4f17-9797-c209a283ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the DB\n",
    "QUERY_TEXT = \"QUESTION\"\n",
    "\n",
    "# embed query\n",
    "query = hf_embedder.get_text_embedding(QUERY_TEXT)\n",
    "\n",
    "# query the vector database\n",
    "index: VectorStoreIndex = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=hf_embedder)\n",
    "retriever = index.as_retriever(similarity_top_k=5, embed_model=hf_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ea26f-e7a7-4f0b-82e4-99c8f3e0d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and print results\n",
    "top_k = retriever.retrieve(QUERY_TEXT)\n",
    "print(f\"Found {len(top_k)} documents\")\n",
    "\n",
    "# display scores\n",
    "for item in top_k:\n",
    "    print(f\"ID: [{item.id_}] - Score: {item.score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
