{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de57e43-7368-41fd-84f7-09c543688f7d",
      "metadata": {
        "id": "4de57e43-7368-41fd-84f7-09c543688f7d"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "# use pip and virtualenv\n",
        "!python --version\n",
        "!pip install nltk matplotlib prettytable tqdm\n",
        "!pip install langchain_community chromadb sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe709c0c-ddc2-40e5-a3dc-da99529485a6",
      "metadata": {
        "id": "fe709c0c-ddc2-40e5-a3dc-da99529485a6"
      },
      "outputs": [],
      "source": [
        "# global vars\n",
        "TRAINING_DATA_PATH = \"training_data.local/\"\n",
        "\n",
        "# import libraries\n",
        "try:\n",
        "    from langchain_core.documents import Document\n",
        "    from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter as rts\n",
        "    from prettytable import PrettyTable\n",
        "    from chromadb.utils import embedding_functions as ef\n",
        "    from chromadb import Client\n",
        "    import uuid\n",
        "    from tqdm.autonotebook import tqdm, trange\n",
        "    from nltk import word_tokenize, sent_tokenize, download\n",
        "    import os\n",
        "    download(\"punkt\")\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception as e:\n",
        "    print(f\"Caught Exception {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Processing Primitives\n",
        "# Embedding function\n",
        "def s_transformer(model: str = \"all-MiniLM-L6-v2\"):\n",
        "    return ef.SentenceTransformerEmbeddingFunction(model_name=model)\n",
        "\n",
        "# load text documents from filesystem\n",
        "def load_text_documents(path: str = \".\", pattern: str = \"**/*.txt\",\n",
        "                        multithread: bool = False) -> list:\n",
        "    loader = DirectoryLoader(path,\n",
        "                             glob=pattern,\n",
        "                             loader_cls=TextLoader,\n",
        "                             loader_kwargs={'autodetect_encoding': True},\n",
        "                             use_multithreading=multithread,\n",
        "                             silent_errors=True,\n",
        "                             show_progress=True)\n",
        "    return loader.load()\n",
        "\n",
        "# prepare the knowledge corpus for further processing\n",
        "def prepare_corpus(raw_loader: list) -> list:\n",
        "    # collect statistics in relation to the data corpus\n",
        "    corpus = []\n",
        "    for doc in raw_loader:\n",
        "        document_tokens = word_tokenize(doc.page_content)\n",
        "        document_sentences = sent_tokenize(doc.page_content)\n",
        "        corpus.append({\"metadata\": doc.metadata,\n",
        "                       \"raw_sentences\": document_sentences,\n",
        "                       \"sentence_count\": len(document_sentences),\n",
        "                       \"wordcount\": len(document_tokens),\n",
        "                       \"vocabulary\": set(document_tokens),\n",
        "                       \"lexical_richness\": len(set(document_tokens)) / len(document_tokens)})\n",
        "\n",
        "    # display corpus\n",
        "    data_table = PrettyTable()\n",
        "    data_table.field_names = [\"Dataset\", \"Word Count\", \"Sentence Count\", \"Vocabulary\", \"Lexical Richness\"]\n",
        "    for dataset in corpus:\n",
        "        data_table.add_row([dataset.get(\"metadata\").get(\"source\"), dataset.get(\"wordcount\"), dataset.get(\"sentence_count\"), len(dataset.get(\"vocabulary\")), dataset.get(\"lexical_richness\")])\n",
        "\n",
        "    # display dataset statistics\n",
        "    print(data_table)\n",
        "\n",
        "    # return processed data\n",
        "    tokenized_data = []\n",
        "    for doc in tqdm(corpus, ascii=True, desc=\"Tokenizing...\"):\n",
        "        metadata = doc.get(\"metadata\")\n",
        "        for data in doc.get(\"raw_sentences\"):\n",
        "            tokenized_data.append(Document(metadata=metadata, page_content=data))\n",
        "\n",
        "    # return corpus and metadata\n",
        "    return tokenized_data, corpus\n",
        "\n",
        "# split corpus in chunks for vectorization\n",
        "def split_text_documents(documents: list = None,\n",
        "                         chunk_size: int = 1000,\n",
        "                         chunk_overlap: int = 0) -> list:\n",
        "    if documents is None:\n",
        "        return None\n",
        "\n",
        "    splitter = rts(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Ogf-URNJTXp1"
      },
      "id": "Ogf-URNJTXp1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed684574-51a4-4b88-bea7-85d1e5361796",
      "metadata": {
        "id": "ed684574-51a4-4b88-bea7-85d1e5361796"
      },
      "outputs": [],
      "source": [
        "# walk the raw data storage path and search for text documents\n",
        "raw_loader = load_text_documents(TRAINING_DATA_PATH)\n",
        "\n",
        "# collect statistics in relation to the data corpus\n",
        "corpus, metadata = prepare_corpus(raw_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e35ad3c-d80f-4a72-8858-6630e3663243",
      "metadata": {
        "id": "0e35ad3c-d80f-4a72-8858-6630e3663243"
      },
      "outputs": [],
      "source": [
        "# plot dataset statistics graph\n",
        "x_vals = [len(x.get(\"vocabulary\")) for x in metadata]\n",
        "y_vals = [100 * x.get(\"lexical_richness\") for x in metadata]\n",
        "y2_vals = [x.get(\"sentence_count\") for x in metadata]\n",
        "\n",
        "# plot vocabulary vs richness data\n",
        "plt.subplot(211)\n",
        "plt.ylabel(\"Lexical Richness\")\n",
        "plt.xlabel(\"Vocabulary\")\n",
        "plt.scatter(x_vals, y_vals)\n",
        "# plot sentence count vs vocabulary\n",
        "plt.subplot(212)\n",
        "plt.ylabel(\"Sentence Count\")\n",
        "plt.xlabel(\"Vocabulary\")\n",
        "plt.scatter(x_vals, y2_vals)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61094bb8-36f9-4410-b463-d643effefb93",
      "metadata": {
        "id": "61094bb8-36f9-4410-b463-d643effefb93"
      },
      "outputs": [],
      "source": [
        "# prepare data for further tokenization\n",
        "CHUNK_SIZE = 1000 # 100 words per chunk\n",
        "OVERLAP = 10 # overlapping words\n",
        "\n",
        "# split documents\n",
        "tokenized_docs = split_text_documents(corpus, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)\n",
        "print(f\"Tokenized {len(tokenized_docs)} documents...\")\n",
        "\n",
        "# remove buffers\n",
        "del(raw_loader)\n",
        "del(corpus)\n",
        "del(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52963f36-8ca8-4f32-9cd2-ac6f8eb8c029",
      "metadata": {
        "id": "52963f36-8ca8-4f32-9cd2-ac6f8eb8c029"
      },
      "outputs": [],
      "source": [
        "# now call the embedding model and upload data to the vector database\n",
        "SIMILARITY_FUNCTION = \"cosine\"\n",
        "COLLECTION_NAME = \"rag_demo\"\n",
        "os.environ['HF_HOME'] = '/tmp/huggingface/hub/'\n",
        "\n",
        "# connect to a running chroma instance\n",
        "try:\n",
        "    vector_store = Client()\n",
        "    chroma_collection = vector_store.get_or_create_collection(COLLECTION_NAME,\n",
        "                                                              metadata={\"hnsw:space\": SIMILARITY_FUNCTION},\n",
        "                                                              embedding_function=s_transformer())\n",
        "except Exception as e:\n",
        "    print(f\"Caught Exception: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60eed6d-ef1a-4152-9845-098379e47bf6",
      "metadata": {
        "id": "f60eed6d-ef1a-4152-9845-098379e47bf6"
      },
      "outputs": [],
      "source": [
        "# embed data and push vectors to the database\n",
        "if len(tokenized_docs) > 0:\n",
        "    for doc in tqdm(tokenized_docs, ascii=True, desc=\"Ingesting...\"):\n",
        "        chroma_collection.add(ids=[str(uuid.uuid1())], documents=doc.page_content, metadatas=doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explore the vector collection\n",
        "print(f\"Objects stored in the collection {chroma_collection.name}: {chroma_collection.count()}\")\n",
        "chroma_collection.query(query_texts=[\"why openshift is better?\"], n_results=2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ziNw2B1vZUUK"
      },
      "id": "ziNw2B1vZUUK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}