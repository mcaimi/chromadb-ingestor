{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de57e43-7368-41fd-84f7-09c543688f7d",
   "metadata": {
    "id": "4de57e43-7368-41fd-84f7-09c543688f7d"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# use pip and virtualenv\n",
    "!python --version\n",
    "!pip install nltk matplotlib prettytable tqdm\n",
    "!pip install langchain_community langchain-huggingface langchain-chroma chromadb==0.5.5 sentence_transformers pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe709c0c-ddc2-40e5-a3dc-da99529485a6",
   "metadata": {
    "id": "fe709c0c-ddc2-40e5-a3dc-da99529485a6"
   },
   "outputs": [],
   "source": [
    "# global vars\n",
    "TRAINING_DATA_PATH = \"training_data.local/\"\n",
    "\n",
    "# import libraries\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter as rts\n",
    "    from langchain_text_splitters import NLTKTextSplitter as nts\n",
    "    from prettytable import PrettyTable\n",
    "    from langchain_chroma import Chroma\n",
    "    from chromadb import Client\n",
    "    import uuid\n",
    "    from tqdm.autonotebook import tqdm, trange\n",
    "    from nltk import word_tokenize, sent_tokenize, download\n",
    "    import os\n",
    "    download(\"punkt\")\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ogf-URNJTXp1",
   "metadata": {
    "id": "Ogf-URNJTXp1"
   },
   "outputs": [],
   "source": [
    "# Text Processing Primitives\n",
    "# Embedding function\n",
    "def s_transformer(model: str = \"all-MiniLM-L6-v2\", hf=False):\n",
    "  if hf is True:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings as hfe\n",
    "    return hfe(model_name=model)\n",
    "  else:\n",
    "    from chromadb.utils import embedding_functions as ef\n",
    "    return ef.SentenceTransformerEmbeddingFunction(model_name=model)\n",
    "\n",
    "# instantiate loader\n",
    "def new_loader(path: str = \".\", pattern: str = \"**/*.txt\",\n",
    "               loader_class=None, l_kwargs: dict = None, multithread: bool = False) -> DirectoryLoader:\n",
    "\n",
    "    if loader_class is None:\n",
    "      loader_class = TextLoader\n",
    "\n",
    "    return DirectoryLoader(path, glob=pattern,\n",
    "                           loader_cls=loader_class,\n",
    "                           loader_kwargs=l_kwargs,\n",
    "                           use_multithreading=multithread,\n",
    "                           silent_errors=True,\n",
    "                           show_progress=True)\n",
    "\n",
    "# load text documents from filesystem\n",
    "def load_text_documents(path: str = \".\", pattern: str = \"**/*.txt\",\n",
    "                        multithread: bool = False) -> list:\n",
    "    loader = new_loader(path=path,\n",
    "                        pattern=pattern,\n",
    "                        loader_class=TextLoader,\n",
    "                        l_kwargs={'autodetect_encoding': True},\n",
    "                        multithread=multithread)\n",
    "    return loader.load()\n",
    "\n",
    "# load PDF documents from filesystem\n",
    "def load_pdf_documents(path: str = \".\", pattern: str = \"**/*.pdf\",\n",
    "                        multithread: bool = False) -> list:\n",
    "    loader = new_loader(path=path,\n",
    "                        pattern=pattern,\n",
    "                        loader_class=PyPDFLoader,\n",
    "                        multithread=multithread)\n",
    "    return loader.load()\n",
    "\n",
    "# prepare the knowledge corpus for further processing\n",
    "def prepare_corpus(raw_loader: list) -> list:\n",
    "    # collect statistics in relation to the data corpus\n",
    "    metadata = []\n",
    "    for doc in raw_loader:\n",
    "        document_tokens = word_tokenize(doc.page_content)\n",
    "        document_sentences = sent_tokenize(doc.page_content)\n",
    "        if (len(document_tokens) != 0) and (len(document_sentences) != 0):\n",
    "          metadata.append({\"metadata\": doc.metadata,\n",
    "                        \"raw_sentences\": document_sentences,\n",
    "                        \"page_contents\": doc.page_content,\n",
    "                        \"sentence_count\": len(document_sentences),\n",
    "                        \"wordcount\": len(document_tokens),\n",
    "                        \"vocabulary\": set(document_tokens),\n",
    "                        \"lexical_richness\": len(set(document_tokens)) / len(document_tokens)})\n",
    "\n",
    "    # display corpus\n",
    "    data_table = PrettyTable()\n",
    "    data_table.field_names = [\"Dataset\", \"Word Count\", \"Sentence Count\", \"Vocabulary\", \"Lexical Richness\"]\n",
    "    for dataset in metadata:\n",
    "        data_table.add_row([dataset.get(\"metadata\").get(\"source\"), dataset.get(\"wordcount\"), dataset.get(\"sentence_count\"), len(dataset.get(\"vocabulary\")), dataset.get(\"lexical_richness\")])\n",
    "\n",
    "    # display dataset statistics\n",
    "    print(data_table)\n",
    "\n",
    "    # return processed data\n",
    "    corpus_data = []\n",
    "    for doc in tqdm(metadata, ascii=True, desc=\"Loading...\"):\n",
    "        doc_metadata = doc.get(\"metadata\")\n",
    "        corpus_data.append(Document(metadata=doc_metadata, page_content=doc.get(\"page_contents\")))\n",
    "\n",
    "    # return corpus and metadata\n",
    "    return corpus_data, metadata\n",
    "\n",
    "# split corpus in chunks for vectorization\n",
    "def split_text_documents_recursive(documents: list = None,\n",
    "                         chunk_size: int = 1000,\n",
    "                         chunk_overlap: int = 0) -> list:\n",
    "    if documents is None:\n",
    "        return None\n",
    "\n",
    "    splitter = rts(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# split corpus using NLTK\n",
    "def split_text_documents_nltk(documents: list = None,\n",
    "                              separator: str = '\\n\\n',\n",
    "                              language: str = 'english') -> list:\n",
    "    if documents is None:\n",
    "      return None\n",
    "\n",
    "    splitter = nts(separator=separator, language=language)\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed684574-51a4-4b88-bea7-85d1e5361796",
   "metadata": {
    "id": "ed684574-51a4-4b88-bea7-85d1e5361796"
   },
   "outputs": [],
   "source": [
    "# walk the raw data storage path and search for text documents\n",
    "raw_text_loader = load_text_documents(TRAINING_DATA_PATH)\n",
    "raw_pdf_loader = load_pdf_documents(TRAINING_DATA_PATH)\n",
    "\n",
    "# collect statistics in relation to the data corpus\n",
    "corpus_data, metadata = prepare_corpus(raw_text_loader + raw_pdf_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35ad3c-d80f-4a72-8858-6630e3663243",
   "metadata": {
    "id": "0e35ad3c-d80f-4a72-8858-6630e3663243"
   },
   "outputs": [],
   "source": [
    "# plot dataset statistics graph\n",
    "x_vals = [len(x.get(\"vocabulary\")) for x in metadata]\n",
    "y_vals = [100 * x.get(\"lexical_richness\") for x in metadata]\n",
    "y2_vals = [x.get(\"sentence_count\") for x in metadata]\n",
    "\n",
    "# plot vocabulary vs richness data\n",
    "plt.subplot(211)\n",
    "plt.ylabel(\"Lexical Richness\")\n",
    "plt.xlabel(\"Vocabulary\")\n",
    "plt.scatter(x_vals, y_vals)\n",
    "# plot sentence count vs vocabulary\n",
    "plt.subplot(212)\n",
    "plt.ylabel(\"Sentence Count\")\n",
    "plt.xlabel(\"Vocabulary\")\n",
    "plt.scatter(x_vals, y2_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61094bb8-36f9-4410-b463-d643effefb93",
   "metadata": {
    "id": "61094bb8-36f9-4410-b463-d643effefb93"
   },
   "outputs": [],
   "source": [
    "# prepare data for further tokenization\n",
    "CHUNK_SIZE = 2000 # 1000 words per chunk\n",
    "OVERLAP = 200 # overlapping words, avoid missing context and splitting sentences\n",
    "SEPARATOR='\\n\\n'\n",
    "TEXT_LANGUAGE='english'\n",
    "\n",
    "# split documents (recursive)\n",
    "tokenized_docs_recursive = split_text_documents_recursive(corpus_data, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)\n",
    "\n",
    "# split documents (NLTK)\n",
    "tokenized_docs_nltk = split_text_documents_nltk(corpus_data, separator=SEPARATOR, language=TEXT_LANGUAGE)\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_docs_recursive)} documents via Recursive Splitter...\")\n",
    "print(f\"Tokenized {len(tokenized_docs_nltk)} documents via NLTK Splitter...\")\n",
    "\n",
    "# remove buffers\n",
    "del(raw_text_loader)\n",
    "del(raw_pdf_loader)\n",
    "del(corpus_data)\n",
    "del(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52963f36-8ca8-4f32-9cd2-ac6f8eb8c029",
   "metadata": {
    "id": "52963f36-8ca8-4f32-9cd2-ac6f8eb8c029"
   },
   "outputs": [],
   "source": [
    "# now call the embedding model and upload data to the vector database\n",
    "SIMILARITY_FUNCTION = \"cosine\"\n",
    "COLLECTION_NAME = \"rag_demo\"\n",
    "os.environ['HF_HOME'] = '/tmp/huggingface/hub/'\n",
    "\n",
    "# connect to a running chroma instance\n",
    "try:\n",
    "    vector_store_client = Client()\n",
    "    chroma_collection = vector_store_client.get_or_create_collection(COLLECTION_NAME,\n",
    "                                                              metadata={\"hnsw:space\": SIMILARITY_FUNCTION})\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQ1O9ASXPGzJ",
   "metadata": {
    "id": "QQ1O9ASXPGzJ"
   },
   "outputs": [],
   "source": [
    "# create a wrapper around the ChromaDB client to use via LangChain\n",
    "chroma_langchain_adapter = Chroma(client=vector_store_client, collection_name=COLLECTION_NAME, embedding_function=s_transformer(hf=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60eed6d-ef1a-4152-9845-098379e47bf6",
   "metadata": {
    "id": "f60eed6d-ef1a-4152-9845-098379e47bf6"
   },
   "outputs": [],
   "source": [
    "# embed data and push vectors to the database (NLTK tokens)\n",
    "if len(tokenized_docs_nltk) > 0:\n",
    "  #chroma_collection.add(ids=[str(uuid.uuid1()) for doc in tokenized_docs], documents=[doc.page_content for doc in tokenized_docs], metadatas=[doc.metadata for doc in tokenized_docs])\n",
    "    for doc in tqdm(tokenized_docs_nltk, ascii=True, desc=\"Ingesting...\"):\n",
    "      chroma_langchain_adapter.add_documents(ids=[str(uuid.uuid1())], documents=[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ziNw2B1vZUUK",
   "metadata": {
    "collapsed": true,
    "id": "ziNw2B1vZUUK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define query example\n",
    "QUERY_TEXT=\"disassembly in x86_64 asm language\"\n",
    "\n",
    "# explore the vector collection\n",
    "print(f\"Objects stored in the collection {chroma_collection.name}: {chroma_collection.count()}\")\n",
    "chroma_collection.query(query_texts=[QUERY_TEXT], n_results=1)\n",
    "\n",
    "# query via LangChain\n",
    "results = chroma_langchain_adapter.similarity_search_with_score(\n",
    "  QUERY_TEXT, k=5,\n",
    ")\n",
    "\n",
    "# build a vector table\n",
    "retrieved_docs = {}\n",
    "for res, score in results:\n",
    "  retrieved_docs[score] = res\n",
    "  print(f\"* {score:3f} - [{res.metadata}]\")\n",
    "\n",
    "# now get the result with the higher similarity (lower score = more similarity)\n",
    "best_doc_by_similarity_score = retrieved_docs[min(retrieved_docs.keys())]\n",
    "print(f\"BEST DOCUMENT - Score {min(retrieved_docs.keys())} - {best_doc_by_similarity_score.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fZYLZ2L4-Qok",
   "metadata": {
    "id": "fZYLZ2L4-Qok"
   },
   "outputs": [],
   "source": [
    "del chroma_langchain_adapter\n",
    "vector_store_client.delete_collection(COLLECTION_NAME)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
